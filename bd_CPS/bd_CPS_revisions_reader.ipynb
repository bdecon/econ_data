{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bd econ CPS extract\n",
    "\n",
    "bd_CPS_revisions_reader.ipynb\n",
    "\n",
    "January 2026 (UPDATE)\n",
    "\n",
    "Contact: Brian Dew, @bd_econ\n",
    "\n",
    "-----\n",
    "\n",
    "Reads in Census revised data and stores it as feather files for merging with the bd CPS. This also separately incldues revised union data.\n",
    "\n",
    "**2000-based revised weights:**\n",
    "\n",
    "Requires: `2000-2extract.txt` data dictionary and unzipped 2000 Based Public Use Extracts from [Census](https://thedataweb.rm.census.gov/ftp/cps_ftp.html#cpsbasic_extract). \n",
    "\n",
    "**2001-2002 revised union data:**\n",
    "\n",
    "Requires: `2000-2extract.txt` data dictionary and unzipped 2000 Based Public Use Extracts from [Census](https://thedataweb.rm.census.gov/ftp/cps_ftp.html#cpsbasic_extract). \n",
    "\n",
    "**2000-2002 recoded occupation and industry:**\n",
    "\n",
    "Requires: `2000-2extract.txt` data dictionary and unzipped 2000 Based Public Use Extracts from [Census](https://thedataweb.rm.census.gov/ftp/cps_ftp.html#cpsbasic_extract). \n",
    "\n",
    "**December 2007 revised weights:**\n",
    "\n",
    "Requires: `dec07revwgts_dd.txt` data dictionary and unzipped `dec07revwgts.dat` from [Census CPS FTP](https://thedataweb.rm.census.gov/ftp/cps_ftp.html). \n",
    "\n",
    "-----\n",
    "\n",
    "Reads in Certification extracts for 2015 and 2016, which identify people with a professional certification, and go into the bd CPS variable CERT.\n",
    "\n",
    "**2015-2016 certification data:**\n",
    "\n",
    "Requires `Certification_extract_file_YYYY_rec_layout.txt` data dictionaries for YYYY in 2015 and 2016, and unzipped data files from [Census CPS FTP](https://thedataweb.rm.census.gov/ftp/cps_ftp.html#cpscert).\n",
    "\n",
    "-----\n",
    "\n",
    "Reads disability flag for late 2008 observations.\n",
    "\n",
    "**June to December 2008 disability flag data:**\n",
    "\n",
    "Requires data dictionaries for YYYY in 2015 and 2016, and unzipped data files from [Census CPS FTP](https://thedataweb.rm.census.gov/ftp/cps_ftp.html#cpsbasic_extract).\n",
    "\n",
    "----\n",
    "\n",
    "**Telework supplement (October 2022 - May 2024):**\n",
    "\n",
    "Requires: Unzipped telework supplement files from [Census CPS FTP](https://thedataweb.rm.census.gov/ftp/cps_ftp.html).\n",
    "\n",
    "----\n",
    "\n",
    "See [related GitHub issue](https://github.com/bdecon/econ_data/issues/82) for explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T21:45:54.307550Z",
     "start_time": "2022-07-15T21:45:54.087428Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas: 2.3.3\n",
      "numpy: 2.3.5\n"
     ]
    }
   ],
   "source": [
    "# Import relevant libraries (python 3.7)\n",
    "import os, re, struct\n",
    "import pandas as pd\n",
    "print('pandas:', pd.__version__)\n",
    "import numpy as np\n",
    "print('numpy:', np.__version__)\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/home/brian/Documents/econ_data/bd_CPS')\n",
    "from bd_CPS_utils import id_dtype, create_struct_unpacker, normalize_weights\n",
    "\n",
    "os.chdir('/home/brian/Documents/CPS/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T21:45:54.316288Z",
     "start_time": "2022-07-15T21:45:54.309147Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:10: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:10: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/tmp/ipykernel_74820/1474305625.py:7: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  p = f'({\"|\".join(var_names)})\\s+(\\d+)\\s+.*? \\s+.*?(\\d+).*?(\\d\\d*)'\n",
      "/tmp/ipykernel_74820/1474305625.py:10: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  p = f'({\"|\".join(var_names)})\\s+(\\d+)\\s+.*?\\t+.*?(\\d+).*?(\\d\\d*)'\n"
     ]
    }
   ],
   "source": [
    "# User-defined functions\n",
    "# Note: id_dtype, create_struct_unpacker, normalize_weights imported from bd_CPS_utils\n",
    "\n",
    "def data_dict_reader(dd_file, var_names):\n",
    "    '''Read data dictionary and return variable locations'''\n",
    "    data_dict = open(dd_file, 'r', encoding='iso-8859-1').read()\n",
    "    p = f'({\"|\".join(var_names)})\\s+(\\d+)\\s+.*? \\s+.*?(\\d+).*?(\\d\\d*)'\n",
    "    if (data_dict[0] == '\\t') & (dd_file == '2000-2extract.txt'):\n",
    "        # Updated data dictionary from Census\n",
    "        p = f'({\"|\".join(var_names)})\\s+(\\d+)\\s+.*?\\t+.*?(\\d+).*?(\\d\\d*)'\n",
    "    d = {s[0]: [int(s[2])-1, int(s[3]), f'{s[1]}s', id_dtype(s[1])]\n",
    "         for s in re.findall(p, data_dict)}\n",
    "    return d\n",
    "\n",
    "def struct_unpacker(d):\n",
    "    '''Return struct unpacker from variable locations'''\n",
    "    return create_struct_unpacker(d, width_has_suffix=True)\n",
    "\n",
    "def data_file_reader(file, unpacker, dtypes, wgt):\n",
    "    '''Convert raw monthly file to dataframe'''\n",
    "    raw_data = open(file, 'rb')\n",
    "    data = [unpacker(row) for row in raw_data]\n",
    "    np_data = np.array(data, dtype=dtypes)\n",
    "    if wgt != 'None':\n",
    "        df = pd.DataFrame(np_data[np_data[wgt] > 0])\n",
    "    else:\n",
    "        df = pd.DataFrame(np_data)\n",
    "    return df\n",
    "\n",
    "def df_adjuster(df, wgt_vars=None):\n",
    "    '''Adjust dataframe to match with bd CPS'''\n",
    "    rev_df = (df.rename({'HRYEAR4': 'YEAR', 'HRMONTH': 'MONTH'}, axis=1)\n",
    "                .assign(YEAR = lambda x: pd.Categorical(x['YEAR'])))\n",
    "    if wgt_vars is not None:\n",
    "        normalize_weights(rev_df, wgt_vars)\n",
    "    return rev_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T06:11:49.548805Z",
     "start_time": "2020-09-02T06:11:49.546902Z"
    }
   },
   "source": [
    "#### Map person weight to HH weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T21:46:07.173261Z",
     "start_time": "2022-07-15T21:45:54.318811Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "/tmp/ipykernel_74820/1331284089.py:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  p = 'D (\\w+)\\s+(\\d{1,2})\\s+(\\d+)\\s+'\n"
     ]
    }
   ],
   "source": [
    "var_names = ['HWHHWGT', 'QSTNUM', 'PWSSWGT', 'OCCURNUM']\n",
    "p = 'D (\\w+)\\s+(\\d{1,2})\\s+(\\d+)\\s+'\n",
    "data_dict = open('jan98dd.asc', 'r', encoding='iso-8859-1').read()\n",
    "d = {s[0]: [int(s[2])-1, int(s[2])+int(s[1])-1, f'{s[1]}s'] \n",
    "     for s in re.findall(p, data_dict) if s[0] in var_names} \n",
    "start, end, width = zip(*d.values())\n",
    "skip = ([f'{s - e}x' for s, e in zip(start, [0] + list(end[:-1]))])\n",
    "unpack_fmt = ''.join([j for i in zip(skip, width) for j in i])\n",
    "unpacker = struct.Struct(unpack_fmt).unpack_from\n",
    "\n",
    "hhw = {2000: {}, 2001: {}, 2002: {}}\n",
    "for date in pd.date_range(start='2000-01-01', end='2002-12-01', freq='MS'):\n",
    "    file = f'{date.strftime(\"%b%y\").lower()}pub.dat'\n",
    "    raw_data = open(file, 'rb').readlines()\n",
    "    wgt = d['PWSSWGT']  \n",
    "    data = [[*map(int, unpacker(row))] for row in raw_data\n",
    "            if int(row[wgt[0]:wgt[1]]) > 0]\n",
    "    df = pd.DataFrame(data, columns=d.keys())\n",
    "    res = (df.query('HWHHWGT == PWSSWGT')\n",
    "             .drop_duplicates('QSTNUM')\n",
    "             [['QSTNUM', 'OCCURNUM']])\n",
    "    hhw[date.year][date.month] = res.set_index('QSTNUM')['OCCURNUM'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Revised 2000-based weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T21:46:15.948170Z",
     "start_time": "2022-07-15T21:46:07.174332Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Store 2000-based revised weights as feather file\n",
    "dd_file = '2000-2extract.txt'\n",
    "\n",
    "var_names = ['HRMONTH', 'HRYEAR4', 'QSTNUM', 'OCCURNUM', \n",
    "             'NWCMPWGT', 'NWORWGT', 'NWSSWGT']\n",
    "\n",
    "wgt_vars = ['NWCMPWGT', 'NWORWGT', 'NWSSWGT']\n",
    "\n",
    "filter_wgt = 'NWSSWGT'\n",
    "\n",
    "d = data_dict_reader(dd_file, var_names)\n",
    "\n",
    "dtypes = [(k, v[-1]) for k, v in d.items()]\n",
    "\n",
    "unpacker = struct_unpacker(d)\n",
    "\n",
    "rev_data_path = 'pubuse2000_2002/'\n",
    "\n",
    "file_names = os.listdir(rev_data_path)\n",
    "\n",
    "files = [rev_data_path + month_file for month_file in file_names]\n",
    "\n",
    "df = pd.concat([data_file_reader(file, unpacker, dtypes, filter_wgt) \n",
    "                for file in files])\n",
    "\n",
    "for year in [2000, 2001, 2002]:\n",
    "    dfy = df.query('HRYEAR4 == @year')\n",
    "\n",
    "    dfy = df_adjuster(dfy, wgt_vars=wgt_vars)\n",
    "    \n",
    "    hhwy = (pd.DataFrame([(k, g, i) for k, v in hhw[year].items() for g, i in v.items()], \n",
    "                    columns=['MONTH', 'QSTNUM', 'HHWLN']))\n",
    "    dfy = dfy.merge(hhwy)\n",
    "    hhws = dfy.query('OCCURNUM == HHWLN')[['MONTH', 'QSTNUM', 'NWSSWGT']].rename({'NWSSWGT': 'NWHHWGT'}, axis=1)\n",
    "    dfy = dfy.merge(hhws).drop('HHWLN', axis=1)\n",
    "\n",
    "    dfy.to_feather(f'clean/cps_wgt_rev{year}.ft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Revised union data (2001-2002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T21:46:20.266945Z",
     "start_time": "2022-07-15T21:46:15.949270Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Store revised union data as feather file\n",
    "dd_file = '2000-2extract.txt'\n",
    "\n",
    "var_names = ['HRMONTH', 'HRYEAR4', 'QSTNUM', 'OCCURNUM', \n",
    "             'NEERNLAB', 'NEERNCOV', 'NWSSWGT']\n",
    "\n",
    "filter_wgt = 'NWSSWGT'\n",
    "\n",
    "d = data_dict_reader(dd_file, var_names)\n",
    "\n",
    "dtypes = [(k, v[-1]) for k, v in d.items()]\n",
    "\n",
    "unpacker = struct_unpacker(d)\n",
    "\n",
    "rev_data_path = 'pubuse2000_2002/'\n",
    "\n",
    "file_names = [file for file in os.listdir(rev_data_path) if file[3:5] != '00']\n",
    "\n",
    "files = [rev_data_path + month_file for month_file in file_names]\n",
    "\n",
    "df = pd.concat([data_file_reader(file, unpacker, dtypes, filter_wgt) \n",
    "                for file in files])\n",
    "\n",
    "for year in [2001, 2002]:\n",
    "    dfy = df.query('HRYEAR4 == @year')\n",
    "\n",
    "    dfy = df_adjuster(dfy).drop('NWSSWGT', axis=1)\n",
    "\n",
    "    dfy.to_feather(f'clean/cps_union_rev{year}.ft')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recoded industry and occupation 2000-2002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T21:46:33.222284Z",
     "start_time": "2022-07-15T21:46:20.268135Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Store 2000-based revised weights as feather file\n",
    "dd_file = '2000-2extract.txt'\n",
    "\n",
    "io_vars = ['NEIO1ICD', 'NEIO2ICD', 'NRDTIND1', 'NRDTIND2', 'NRDTOCC1', \n",
    "           'NRDTOCC2', 'NRMJIND1', 'NRMJIND2', 'NRMJOCC1', 'NRMJOCC2', \n",
    "           'NTIO1OCD', 'NTIO2OCD']\n",
    "\n",
    "var_names = ['HRMONTH', 'HRYEAR4', 'QSTNUM', 'OCCURNUM', 'NWSSWGT'] + io_vars\n",
    "\n",
    "filter_wgt = 'NWSSWGT'\n",
    "\n",
    "d = data_dict_reader(dd_file, var_names)\n",
    "\n",
    "# Manually fixing -- bug to be resolved later\n",
    "d['NEIO1ICD'] = [13, 17, '4s', 'int16']\n",
    "\n",
    "\n",
    "dtypes = [(k, v[-1]) for k, v in d.items()]\n",
    "\n",
    "unpacker = struct_unpacker(d)\n",
    "\n",
    "rev_data_path = 'pubuse2000_2002/'\n",
    "\n",
    "file_names = os.listdir(rev_data_path)\n",
    "\n",
    "files = [rev_data_path + month_file for month_file in file_names]\n",
    "\n",
    "df = pd.concat([data_file_reader(file, unpacker, dtypes, filter_wgt) \n",
    "                for file in files])\n",
    "\n",
    "df = df.drop(['NWSSWGT'], axis=1)\n",
    "\n",
    "\n",
    "df = df.rename({'NEIO1ICD': 'IND02', 'NEIO2ICD': 'IND202', 'NTIO1OCD': 'OCC00',\n",
    "                'NRDTIND1': 'IND03D', 'NRDTIND2': 'IND203D', 'NTIO2OCD': 'OCC200',\n",
    "                'NRDTOCC1': 'OCC03D', 'NRDTOCC2': 'OCC203D', 'NRMJIND1': 'IND03M',\n",
    "                'NRMJIND2': 'IND203M', 'NRMJOCC1': 'OCC03M', 'NRMJOCC2': 'OCC203M'}, axis=1)\n",
    "\n",
    "\n",
    "for year in [2000, 2001, 2002]:\n",
    "    dfy = df.query('HRYEAR4 == @year')\n",
    "\n",
    "    dfy = df_adjuster(dfy)\n",
    "\n",
    "    dfy.to_feather(f'clean/cps_io_rev{year}.ft')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Revised December 2007 weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T21:46:33.394845Z",
     "start_time": "2022-07-15T21:46:33.223528Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Store revised union data as feather file\n",
    "dd_file = 'dec07revwgts_dd.txt'\n",
    "\n",
    "var_names = ['QSTNUM', 'OCCURNUM', 'PWSSWGT', 'PWCMPWGT']\n",
    "\n",
    "filter_wgt = 'PWSSWGT'\n",
    "\n",
    "d = data_dict_reader(dd_file, var_names)\n",
    "\n",
    "dtypes = [(k, v[-1]) for k, v in d.items()]\n",
    "\n",
    "unpacker = struct_unpacker(d)\n",
    "\n",
    "file = 'dec07revwgts.dat'\n",
    "\n",
    "# Special code to remove rows with only '.'\n",
    "with open(file, 'rb+') as f:\n",
    "    new_f = f.readlines()\n",
    "    f.seek(0)\n",
    "    for line in new_f:\n",
    "        if b'.' not in line:\n",
    "            f.write(line)\n",
    "    f.truncate()\n",
    "\n",
    "df = data_file_reader(file, unpacker, dtypes, filter_wgt) \n",
    "\n",
    "normalize_weights(df, ['PWSSWGT', 'PWCMPWGT'])\n",
    "\n",
    "df = df.rename({'PWSSWGT': 'NWSSWGT', 'PWCMPWGT': 'NWCMPWGT'}, axis=1)\n",
    "\n",
    "df.reset_index(drop=True).to_feather('clean/cps_dec07_rev.ft')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Certification data for 2015-2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T21:46:35.181887Z",
     "start_time": "2022-07-15T21:46:33.395938Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Store certification data as feather file\n",
    "\n",
    "# Use the 2016 dictionary for both years\n",
    "dd_file = 'Certification_extract_file_2016_rec_layout.txt'\n",
    "\n",
    "var_names = ['QSTNUM', 'PULINENO', 'MONTH', 'PECERT1']\n",
    "\n",
    "filter_wgt = 'PECERT1'\n",
    "\n",
    "d = data_dict_reader(dd_file, var_names)\n",
    "\n",
    "dtypes = [(k, v[-1]) for k, v in d.items()]\n",
    "\n",
    "unpacker = struct_unpacker(d)\n",
    "\n",
    "# Loop over two years and create feather for each\n",
    "for year in ['2015', '2016']:\n",
    "    file = f'jan{year[2:]}-dec{year[2:]}cert_ext.dat'\n",
    "\n",
    "    df = data_file_reader(file, unpacker, dtypes, filter_wgt) \n",
    "\n",
    "    (df.reset_index(drop=True)\n",
    "       .rename({'PULINENO': 'LINENO'}, axis=1)\n",
    "       .to_feather(f'clean/cps_cert{year}.ft'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disability Flag late 2008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T21:46:35.714143Z",
     "start_time": "2022-07-15T21:46:35.183563Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Store disability data as feather file\n",
    "\n",
    "d = {'QSTNUM': [0, 8, '8s', 'int32'],\n",
    "     'HRMONTH': [8, 16, '8s', 'int8'],\n",
    "     'OCCURNUM': [24, 32, '8s', 'int32'],\n",
    "     'PRDISFLG': [80, 88, '8s', 'int32']}\n",
    "dtypes = [(k, v[-1]) for k, v in d.items()]\n",
    "\n",
    "unpacker = struct_unpacker(d)\n",
    "\n",
    "# Unpack data\n",
    "file = 'disability.dat'\n",
    "\n",
    "df = data_file_reader(file, unpacker, dtypes, 'QSTNUM') \n",
    "\n",
    "df = df.rename({'HRMONTH': 'MONTH'}, axis=1)\n",
    "\n",
    "df.reset_index(drop=True).to_feather(f'clean/cps_disability2008.ft')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T16:50:39.389414Z",
     "start_time": "2020-11-21T16:50:39.387255Z"
    }
   },
   "source": [
    "#### Telework Supplement Data (October 2022 - May 2024)\n",
    "\n",
    "Monthly telework supplement files stored in Telework/ subfolder. Starting June 2024,\n",
    "telework questions were folded into the main CPS survey.\n",
    "\n",
    "Only two variables are used by bd CPS:\n",
    "- PTCOVR1: Telework status (1=yes, 2=no)\n",
    "- PTCOVR2: Hours teleworked\n",
    "\n",
    "Requires: Unzipped telework supplement files from [Census CPS FTP](https://thedataweb.rm.census.gov/ftp/cps_ftp.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T21:46:36.967307Z",
     "start_time": "2022-07-15T21:46:35.715193Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  apr23cpucvr_pub.dat: 45,907 records\n",
      "  apr24cpucvr_pub.dat: 45,965 records\n",
      "  aug23cpucvr_pub.dat: 46,000 records\n",
      "  dec22cpucvr_pub.dat: 45,113 records\n",
      "  dec23cpucvr_puf.dat: 45,690 records\n",
      "  feb23cpucvr_pub.dat: 44,568 records\n",
      "  feb24cpucvr_puf.dat: 45,768 records\n",
      "  jan23cpucvr_pub.dat: 45,200 records\n",
      "  jan24cpucvr_puf.dat: 45,154 records\n",
      "  jul23cpucvr_pub.dat: 45,094 records\n",
      "  jun23cpucvr_pub.dat: 45,540 records\n",
      "  mar23cpucvr_pub.dat: 43,831 records\n",
      "  mar24cpucvr_pub.dat: 43,918 records\n",
      "  may23cpucvr_pub.dat: 46,743 records\n",
      "  may24cpucvr_puf.dat: 46,029 records\n",
      "  nov22cpucvr_pub.dat: 45,653 records\n",
      "  nov23cpucvr_puf.dat: 46,310 records\n",
      "  oct22cpucvr_pub.dat: 46,319 records\n",
      "  oct23cpucvr_pub.dat: 47,135 records\n",
      "  sep23cpucvr_pub.dat: 46,112 records\n",
      "Saved cps_telework2022.ft: 137,085 records (months: [np.int8(10), np.int8(11), np.int8(12)])\n",
      "Saved cps_telework2023.ft: 548,130 records (months: [np.int8(1), np.int8(2), np.int8(3), np.int8(4), np.int8(5), np.int8(6), np.int8(7), np.int8(8), np.int8(9), np.int8(10), np.int8(11), np.int8(12)])\n",
      "Saved cps_telework2024.ft: 226,834 records (months: [np.int8(1), np.int8(2), np.int8(3), np.int8(4), np.int8(5)])\n"
     ]
    }
   ],
   "source": [
    "# Store telework supplement data as feather files\n",
    "# Supplement ran Oct 2022 - May 2024; only PTCOVR1 and PTCOVR2 are used\n",
    "\n",
    "telework_path = 'Telework/'\n",
    "all_telework = []\n",
    "\n",
    "# Fixed-width format: same positions in both V1 (30-byte) and V2 (22-byte) files\n",
    "# QSTNUM: 1-5, OCCURNUM: 6-7, HRMONTH: 8-9, HRYEAR: 10-13, PTCOVR1: 14-15, PTCOVR2: 16-18\n",
    "widths = [5, 2, 2, 4, 2, 3]\n",
    "columns = ['QSTNUM', 'OCCURNUM', 'HRMONTH', 'HRYEAR', 'PTCOVR1', 'PTCOVR2']\n",
    "\n",
    "for filename in sorted(os.listdir(telework_path)):\n",
    "    if not filename.endswith('.dat'):\n",
    "        continue\n",
    "    filepath = telework_path + filename\n",
    "\n",
    "    data = []\n",
    "    with open(filepath, 'rb') as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip(b'\\r\\n')\n",
    "            if len(line) == 0:\n",
    "                continue\n",
    "            pos = 0\n",
    "            row = []\n",
    "            for width in widths:\n",
    "                field = line[pos:pos + width].decode('ascii').strip()\n",
    "                try:\n",
    "                    row.append(int(field))\n",
    "                except ValueError:\n",
    "                    row.append(np.nan)\n",
    "                pos += width\n",
    "            data.append(row)\n",
    "\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    all_telework.append(df)\n",
    "    print(f\"  {filename}: {len(df):,} records\")\n",
    "\n",
    "# Combine and process\n",
    "if all_telework:\n",
    "    combined = pd.concat(all_telework, ignore_index=True)\n",
    "    combined = combined.rename(columns={'HRMONTH': 'MONTH', 'HRYEAR': 'YEAR'})\n",
    "    combined.loc[combined['YEAR'] < 100, 'YEAR'] += 2000\n",
    "\n",
    "    # Set dtypes (use nullable Int for PTCOVR2 since it can have NaN)\n",
    "    combined = combined.astype({\n",
    "        'QSTNUM': 'int32', 'OCCURNUM': 'int8', 'MONTH': 'int8', 'YEAR': 'int16',\n",
    "        'PTCOVR1': 'int8', 'PTCOVR2': 'Int16'\n",
    "    })\n",
    "\n",
    "    # Save by year\n",
    "    for year in sorted(combined['YEAR'].unique()):\n",
    "        year_data = combined[combined['YEAR'] == year].drop('YEAR', axis=1)\n",
    "        year_data.reset_index(drop=True).to_feather(f'clean/cps_telework{year}.ft')\n",
    "        months = sorted(year_data['MONTH'].unique())\n",
    "        print(f\"Saved cps_telework{year}.ft: {len(year_data):,} records (months: {months})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
